{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "# Our helper, with the functions: \n",
    "# plot_vector, plot_linear_transformation, plot_linear_transformations\n",
    "from plot_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry of eigendecomposition\n",
    "\n",
    "When you know the eigenvectors and eigenvalues, you know the matrix. Many applications of linear algebra benefit from expressing a matrix using its eigenvectors and eigenvalues (it often makes computations easier). \n",
    "This is called _eigendecomposition_—matrix decomposition is a central topic in linear algebra, and particularly important in applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors revisited\n",
    "\n",
    "In the previous lesson, we saw that a unit circle, by a 2D linear transformation, lands on an ellipse. The semi-major and semi-minor axes of the ellipse are in the direction of the eigenvectors of the transformation matrix. Let's revisit that.\n",
    "\n",
    "\n",
    "We'll work with the matrix $\\,A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = numpy.array([[2,1], [1,2]])\n",
    "print(A)\n",
    "plot_linear_transformation(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous lesson, plot a set of vectors of unit length (whose heads trace the unit circle), visualize the transformed vectors, then compute the length of the semi-major and semi-minor axes of the ellipse (the norm of the longest and shortest vectors in our set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = numpy.linspace(0, 2*numpy.pi, 41)\n",
    "vectors = list(zip(numpy.cos(alpha), numpy.sin(alpha)))\n",
    "newvectors = []\n",
    "for i in range(len(vectors)):\n",
    "    newvectors.append(A.dot(numpy.array(vectors[i])))\n",
    "\n",
    "plot_vector(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vector(newvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for i in range(len(newvectors)):\n",
    "    lengths.append(numpy.linalg.norm(newvectors[i]))\n",
    "semi_major = max(lengths)\n",
    "print('Semi-major axis {:.1f}'.format(semi_major))\n",
    "semi_minor = min(lengths)\n",
    "print('Semi-minor axis {:.1f}'.format(semi_minor))\n",
    "\n",
    "u1 = numpy.array([semi_major/numpy.sqrt(2), semi_major/numpy.sqrt(2)])\n",
    "u2 = numpy.array([-semi_minor/numpy.sqrt(2), semi_minor/numpy.sqrt(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A scaling transformation\n",
    "\n",
    "OK, cool. In our first lesson, we saw some special transformations: _rotation_, _shear_, and _scaling_. \n",
    "Looking at the effect of the matrix transformation $A$ on the unit circle, we might imagine obtaining the same effect by first scaling the unit vectors—stretching $\\mathbf{i}$ to $3\\times$ its length and leaving $\\mathbf{j}$ with length $1$—and then rotating by 45 degrees counter-clockwise.\n",
    "We have also learned that applying linear transformations in sequence like this amounts to matrix multiplication.\n",
    "\n",
    "Let's try it. We first define the scaling transformation $S$, and apply it to the vectors mapping the unit circle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = numpy.array([[3,0], [0,1]])\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipse = []\n",
    "for i in range(len(vectors)):\n",
    "    ellipse.append(S.dot(numpy.array(vectors[i])))\n",
    "\n",
    "plot_vector(ellipse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A rotation transformation\n",
    "\n",
    "We figured out the matrix for a 90-degree rotation in our first lesson. But how do you rotate by any angle? You never have to memorize the \"formula\" for a rotation matrix. Just think about where the unit vectors land. Look at the figure below, and follow along on a piece of paper if you need to.\n",
    "\n",
    "<img src=\"../images/rotation.png\" style=\"width: 300px;\"/> \n",
    "#### Rotation of unit vectors by an angle $\\theta$ to the left.\n",
    "\n",
    "$$\n",
    "\\mathbf{i} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}  \\Rightarrow  \\begin{bmatrix} \\cos{\\theta} \\\\ \\sin{\\theta} \\end{bmatrix} \\\\\n",
    "\\mathbf{j} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}  \\Rightarrow  \\begin{bmatrix} -\\sin{\\theta} \\\\ \\cos{\\theta} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You now can build the rotation matrix using the column vectors where each unit vector lands.\n",
    "\n",
    "$$R = \\begin{bmatrix} \\cos{\\theta} & -\\sin{\\theta} \\\\ \\sin{\\theta} & \\cos{\\theta} \\end{bmatrix}$$\n",
    "\n",
    "Great. Let's define a matrix $R$ that rotates vectors by 45 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = numpy.pi/4\n",
    "R = numpy.array([[numpy.cos(theta), -numpy.sin(theta)], \n",
    "                 [numpy.sin(theta), numpy.cos(theta)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this rotation now to the `ellipse` vectors, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated = []\n",
    "for i in range(len(vectors)):\n",
    "    rotated.append(R.dot(numpy.array(ellipse[i])))\n",
    "\n",
    "plot_vector(rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of the transformations\n",
    "\n",
    "It certainluy _looks_ like we recovered the picture we obtained originally when applying the transformation $A$ to all our vectors on the unit circle.  \n",
    "\n",
    "But have a look at the two transformations—the scaling $S$ and the rotation $R$—applied in sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_transformations(S,R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe carefully the plot above. The scaling did stretch the basis vector $\\mathbf{i}$ by $3\\times$ its original length. It also left the basis vector $\\mathbf{j}$ with its length equal to $1$. But something looks _really_ off after the second transformation. \n",
    "\n",
    "We know from the discussion in the previous lesson that the vector that lands on the ellipse's semi-major axis doesn't change direction. It's _not_ the basis vector $\\mathbf{i}$ that lands there, it's the vector $\\mathbf{v}_1$ that satisfies: \n",
    "\n",
    "$$ A \\mathbf{v}_1 = s_1 \\mathbf{v}_1 $$\n",
    "\n",
    "Recalling the process we followed in the previous lesson, we find that vector, and plot it together with its transformed version (also called its _image_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_inv = numpy.linalg.inv(A)\n",
    "v1 = A_inv.dot(u1)\n",
    "plot_vector([u1,v1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right. The unit vector that was aligned with the 45-degree line got transformed onto the semi-major axis of the ellipse, without being rotated. This is the effect of the matrix $A$ on $\\mathbf{v}_1$: _it is just scaled_.\n",
    "\n",
    "Now, let's look at the sequence of transformations $S$ and $R$ applied to $\\mathbf{v}_1$. We apply the transformations by matrix-vector multiplication, and in the second step, we use composition of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vector([v1, S.dot(v1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vector([S.dot(v1),R.dot(S.dot(v1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is definitely _not_ what we expected. Oh well. It seemed like a good idea at the time, but the scaling $S$ and the rotation $R$ applied in sequence are _not_ equivalent to the transformation $A$. \n",
    "Our visual intuition was not able to get the whole picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the composition\n",
    "\n",
    "OK. This will blow your mind… to get the same transformation as $A$ we had to _first_ rotate 45 degrees to the right (which leaves the plot of our circle unchanged even though the vectors rotated), _then_ scale, and finally rotate 45 degrees to the left. \n",
    "\n",
    "We will look at this sequence of transformations via matrix multiplicaton. But first note that a rotation by a negative angle $\\theta$ is achieved by the matrix:\n",
    "\n",
    "$$R^T = \\begin{bmatrix} \\cos{\\theta} & \\sin{\\theta} \\\\ -\\sin{\\theta} & \\cos{\\theta} \\end{bmatrix}$$\n",
    "\n",
    "Check using a piece of paper that the columns of this matrix make sense for a negative angle $\\theta$, and notice that it is the transpose of $R$ (i.e., swaps rows and columns).\n",
    "\n",
    "Now look at the result of the matrix multiplication (equivalent to the transformations in sequence, right to left):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R @ S @ R.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's certainly the same as $A$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_transformation(R @ S @ R.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some explaining to do. Let's visualize the transformation $R^T$, adding to our plot the unit vectors that were aligned with the eigenvectors, $\\mathbf{v}_1$ and $\\mathbf{v}_2$. You see that they land on the coordinate axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = A_inv.dot(u2)\n",
    "plot_linear_transformation(R.transpose(), v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize applying  the scaling transformation to these vectors, and applying the rotation matrix $R$ after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = R.transpose().dot(v1)\n",
    "e2 = R.transpose().dot(v2)\n",
    "\n",
    "plot_linear_transformation(S, e1, e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_transformation(R, S.dot(e1), S.dot(e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Satisfied? The vectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$ are first rotated to land on the axes, are then scaled, and are finally rotated back to their original direction. This has the same effect as the transformation $A$. In other words:\n",
    "\n",
    "\n",
    "$$ A = R\\, S\\, R^T \n",
    "$$\n",
    "\n",
    "The equivalence above shows an **eigendecomposition** of the matrix $A$. \n",
    "The scaling matrix $S$ has the *eigenvalues* along the diagonal, and the rotation matrix $R$ has the *eigenvectors* as columns. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R[:,0])\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R[:,1])\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric matrices, orthogonal eigenvectors\n",
    "\n",
    "In our example, we figured out that the matrix that undoes the effect of the positive rotation $R$ is its transpose, $R^T$. But in general, the matrix that has the effect of undoing a transformation is the _inverse_ of this matrix. In fact, $R$ is special, because\n",
    "\n",
    "$$ R^T = R^{-1} $$\n",
    "\n",
    "##### Definition:\n",
    "\n",
    "> A matrix $R$ whose transpose is equal to its inverse is called **orthogonal**.\n",
    "\n",
    "The columns of an orthogonal matrix are orthogonal vectors of unit length (a.k.a., _orthonormal_). For our example, it means that the _eigenvectors of $A$ are orthogonal_.\n",
    "This always happens with _symmetric matrices_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy.linalg.inv(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check visually that $R^T$ is equal to $R^{-1}$—but if we try a logical expression, it will give `False` because of floating-point errors. To check for the symmetry of $A$, we can use a logical operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A == A.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to confirm that the two eigenvectors are orthogonal, we take the dot product, which is zero for perpendicular vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key idea:\n",
    "\n",
    "> A _symmetric_ matrix, $\\,A = A^T$, has _orthogonal_ eigenvectors, and can be decomposed as $A = R\\, S\\, R^T$, where $R$ has the eigenvectors as columns and $S$ is the diagonal matrix of eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition in general\n",
    "\n",
    "Let's work the general case. For the two eigenvectors of $A$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  A \\mathbf{v_1} = s_1 \\mathbf{v_1} \\\\\n",
    "  A \\mathbf{v_2} = s_2 \\mathbf{v_2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The two left-hand sides are matrix-vector multiplications, giving a vector as result. They can be arranged as two column vectors. \n",
    "The two right-hand sides are a scalar multiplying a column vector.\n",
    "Stack the two equations together like this: \n",
    "\n",
    "$$\n",
    "  A \\begin{bmatrix}\n",
    "    \\mathbf{v_1} & \\mathbf{v_2}\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1} & \\mathbf{v_2}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    s_1 & 0 \\\\\n",
    "    0 & s_2\n",
    "    \\end{bmatrix}  \n",
    "$$\n",
    "\n",
    "Using $C$ to denote the matrix of eigenvectors, and $D$ to denote the diagonal matrix of eigenvalues, it becomes:\n",
    "\n",
    "$$\n",
    "  A\\, C = C\\, D\n",
    "$$\n",
    "\n",
    "then right-multiply by $C^{-1}$ on both sides:\n",
    "\n",
    "$$\n",
    "  A = C\\, D\\, C^{-1}\n",
    "$$\n",
    "\n",
    "A matrix that can be decomposed in this way is called **diagonalizable**.\n",
    "Multiplying on the right by $C$ and on the left by $C^{-1}$:\n",
    "\n",
    "$$\n",
    "  C^{-1} A\\, C = D\n",
    "$$\n",
    "\n",
    "\n",
    "If you go back to the previous lesson, you will find an expression that looks like this for applying a known transformation to a vector in a new basis. Review that section if you need to.\n",
    "\n",
    "Applying the transformation $A$ to a vector in standard basis, $\\mathbf{x}$, is:\n",
    "\n",
    "$$\n",
    "  A\\, \\mathbf{x} = C\\, D\\, C^{-1}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "Viewing $C$ as a change of basis, the expression on the right means we change $\\mathbf{x}$ to a new basis of eigenvectors, apply a scaling by the eigenvalues in the new coordinate system, and change back to the standard basis. The effect is the same as the transformation $A$: the matrices $A$ and $D$ are called **similar**.\n",
    "\n",
    "##### Key idea:\n",
    "\n",
    "> Similar matrices have the same effect, via a change of basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute eigenthings in Python\n",
    "\n",
    "You can compute the eigenvalues and eigenvectors of a matrix using [`numpy.linalg.eig()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html). It returns a tuple: its first element is an array with the eigenvalues, and its second element is a 2D array  where each column is an eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.linalg.eig(A)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.linalg.eig(A)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display each eigenvalue with the corresponding eigenvector, side by side\n",
    "eigenvalues, eigenvectors = numpy.linalg.eig(A)\n",
    "\n",
    "for eigenvalue, eigenvector in zip(eigenvalues, eigenvectors.T):\n",
    "    print(eigenvalue, eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the diagonal matrix of eigenvalues, use [`numpy.diag()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.diag.html): if you give it a 1D array, it returns a 2D array with the elements of the input array in the diagonal. \n",
    "\n",
    "The eigendecomposition is shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = eigenvectors\n",
    "A_decomp = C @ numpy.diag(eigenvalues) @ numpy.linalg.inv(C)\n",
    "print(A_decomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to compute all the eigenthings is to use **SymPy**: the Python library for symbolic computations (a.k.a., computer algebra system). SymPy has a [`Matrix`](https://docs.sympy.org/latest/tutorial/matrices.html) data type with many advanced methods. \n",
    "\n",
    "You will create a `Matrix` from a list of row vectors, and then use the `diagonalize()` method to obtain the matrix of eigenvectors, and the diagonal matrix of eigenvalues. \n",
    "SymPy can display beautiful symbolic mathematics. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "\n",
    "sympy.init_printing(use_latex = 'mathjax') #configures the display of mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sympy.Matrix([[2,1], [1,2]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A == A.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, D = A.diagonalize()\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SymPy, you can also get just the eigenvalues using the [`eigenvals()`](https://docs.sympy.org/latest/tutorial/matrices.html#eigenvalues-eigenvectors-and-diagonalization) method. Its output will be in the form $\\{s_1:m_1, s_2:m_2\\}$, where $s_1$, $s_2$ are the eigenvalues of the matrix, and $m_1$, $m_2$ are the _multiplicities_. A matrix can have (multiply) repeated eigenvalues sometimes. In the case of our matrix $A$, both eigenvalues are unique (i.e., have multiplicity $1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.eigenvals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But think of the identity matrix: it has $1$ along the diagonal, and the two eigenvalues are the same, $1$. We can create an identity matrix in SymPy using `eye(n)`, with `n` indicating the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.eye(2).eigenvals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try with the _shear_ matrix from our fist lesson, we see that it also has a repeated $1$ eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear = sympy.Matrix([[1,1], [0,1]])\n",
    "shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear.eigenvals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look back at the equation for the eigenvectors—on which the effect of a matrix $A$ is just to scale them—and rearrange things a bit:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "  A\\, \\mathbf{v} &= s\\, \\mathbf{v}\\\\\n",
    "  A\\, \\mathbf{v} - s\\, \\mathbf{v} &= \\mathbf{0} \\\\\n",
    "  (A - s\\, I) \\mathbf{v} &= \\mathbf{0}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The last form represents a homogeneous linear system, whose solutions are the _nullspace_ of $(A-s\\,I)$, as we saw in our second lesson. We can compute the nullspace using SymPy; let's try it with the shear matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(shear - sympy.eye(2)).nullspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`nullspace()`](https://docs.sympy.org/latest/tutorial/matrices.html#nullspace) method of SymPy matrices returns a list of column vectors that span the nullspace of the matrix. In the case of the shear matrix, only one column vector is returned, which means  its nullspace is a line.\n",
    "\n",
    "The shear matrix has one repeated eigenvalue, and a single eigenvector: we cannot build a change of basis with its eigenvectors, which means it's _not diagonalizable_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
